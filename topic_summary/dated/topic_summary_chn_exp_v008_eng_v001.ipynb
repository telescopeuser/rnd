{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese text summarization algorithm\n",
    "### www.KudosData.com\n",
    "#### By: Sam GU Zhan\n",
    "#### March, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=UTF-8\n",
    "from __future__ import division\n",
    "import re\n",
    "\n",
    "# Python2 unicode & float-division support:\n",
    "# from __future__ import unicode_literals, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "\n",
    "# 中文字符和语言处理库\n",
    "import jieba\n",
    "\n",
    "# 机器学习库 sklearn 分类学习模型库\n",
    "#from sklearn import linear_model\n",
    "from sklearn.feature_extraction import DictVectorizer # 数据结构变换：把 Dict 转换为 稀疏矩阵\n",
    "# from sklearn.linear_model import LogisticRegression  # 逻辑回归分类模型\n",
    "# from sklearn.pipeline import make_pipeline # 封装机器学习模型流程\n",
    "# from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# 中文显示设置\n",
    "from pylab import *  \n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei'] # 指定默认字体  \n",
    "mpl.rcParams['axes.unicode_minus'] = False # 解决保存图像是负号'-'显示为方块的问题 \n",
    "mpl.rcParams['font.size'] = 14 # 设置字体大小\n",
    "\n",
    "np.random.seed(88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python3\n",
    "# 中文分词功能小函数， 输出 字符串， 各词组由空格分隔\n",
    "def KudosData_word_tokenizer(foo):\n",
    "    # remove lead & tail spaces firstly:\n",
    "    foo = foo.strip()\n",
    "    seg_token = jieba.cut(str(foo), cut_all=True)\n",
    "    seg_str = str(' '.join(seg_token)).strip()\n",
    "\n",
    "    return seg_str\n",
    "# Python2\n",
    "# 中文分词功能小函数， 输出 字符串， 各词组由空格分隔\n",
    "# def KudosData_word_tokenizer(foo):\n",
    "#     seg_token = jieba.cut(foo, cut_all=True)\n",
    "#     seg_str = ' '.join(seg_token)\n",
    "#     return seg_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python3\n",
    "# 中文分词功能小函数， 输出 字符串， 各词组由空格分隔\n",
    "def KudosData_word_count(foo):\n",
    "    # remove lead & tail spaces firstly:\n",
    "    foo = foo.strip()\n",
    "    seg_token = jieba.cut(str(foo), cut_all=True)\n",
    "    seg_str = str(' '.join(seg_token)).strip()\n",
    "    seg_count = pd.value_counts(str(seg_str).lower().split(' '))\n",
    "    seg_count = seg_count.to_dict() \n",
    "    seg_count.pop('', None) # remove EMPTY dict key: ''\n",
    "#     输出 dictionary： { key 词组， value 计数 }\n",
    "    #     return seg_count.to_dict()\n",
    "    return seg_count\n",
    "\n",
    "# Python2\n",
    "# 中文分词功能小函数， 输出 dictionary： { key 词组， value 计数 }\n",
    "# def KudosData_word_count(foo):\n",
    "#     seg_token = jieba.cut(foo, cut_all=True)\n",
    "#     seg_str = '^'.join(seg_token)\n",
    "#     seg_count = pd.value_counts(seg_str.lower().split('^'))\n",
    "#     return seg_count.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# process Unicode text input\n",
    "with io.open('input_text.txt','r',encoding='utf8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "title = '''\n",
    "<Dummy Title>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_sentence(text):\n",
    "#     sentence = re.sub(r'\\W+', '#', sentence)\n",
    "    text = re.sub(r'\\t+', '', text) # remove one or more Tab\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linebreak_conversion_win_linux(text):\n",
    "    text = re.sub(r'\\r', '', text) # remove one or more Windows-line-break\n",
    "    text = re.sub(r'\\u3000', ' ', text) # convert white space: \\u3000    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_some_whitespace_1(text): # Does not remove normal Space\n",
    "#     sentence = re.sub(r'\\W+', '#', sentence)\n",
    "    text = re.sub(r'\\t+', '', text) # remove one or more Tab\n",
    "    text = re.sub(r'\\f+', '', text) # remove one or more special Space\n",
    "    text = re.sub(r'\\v+', '', text) # remove one or more special Space\n",
    "    text = re.sub(r' +', ' ', text) # merge two or more Spaces to 1 Space\n",
    "    \n",
    "    # remove lead & tail spaces:\n",
    "    text =text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_some_whitespace_2(text): # Does not remove normal Space\n",
    "#     sentence = re.sub(r'\\W+', '#', sentence)\n",
    "    text = re.sub(r'\\n+', '', text) # remove one or more \\n, this is to merge sentences within paragraph\n",
    "    text = re.sub(r' +', ' ', text) # merge two or more Spaces to 1 Space\n",
    "    text = re.sub(r'(\\^\\*\\#)( +)(\\#\\*\\^)', '^*##*^', text) # remove one or more Spaces between Paragraph-Tags or Sentence-Tags\n",
    "\n",
    "    # remove lead & tail spaces:\n",
    "    text =text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define Paragraph-Tag =  \n",
    "#   #*^P^*#\n",
    "\n",
    "### Define Sentence-Tag =  \n",
    "#   #*^S^*#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a special tag to end of each paragraph\n",
    "def tag_paragraph(text):\n",
    "    text = re.sub(r'((\\n ) +)+', '#*^P^*#', text) # Tag paragraph, pattern: \\n + two or more Spaces\n",
    "    text = re.sub(r'((\\n\\t) +)+', '#*^P^*#', text) # Tag paragraph, pattern: \\n + two or more Tabs\n",
    "    text = re.sub(r'(\\n( *)\\n)+', '#*^P^*#', text) # Tag paragraph, pattern: \\n + zero or more Spaces + \\n\n",
    "    text = re.sub(r'(\\#\\*\\^P\\^\\*\\#)+', '#*^P^*#', text) # merge two or more Paragraph-Tags -> 1 Paragraph-Tag\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a special tag to end of each sentence\n",
    "def tag_sentence(text):\n",
    "    text = re.sub(r'。+', '。#*^S^*#', text) # Tag sentence - Chinese\n",
    "    text = re.sub(r'！+', '！#*^S^*#', text) # Tag sentence - Chinese\n",
    "    text = re.sub(r'\\？+', '？#*^S^*#', text) # Tag sentence - Chinese\n",
    "#     text = re.sub(r'；+', '；#*^S^*#', text) # Tag sentence - Chinese\n",
    "\n",
    "    # 2017 MAR 24\n",
    "    text = re.sub(r'(\\.)( +)', '.#*^S^*#', text) # Tag sentence - English\n",
    "    text = re.sub(r'(!)( +)', '!#*^S^*#', text) # Tag sentence - English\n",
    "    text = re.sub(r'\\?( +)', '?#*^S^*#', text) # Tag sentence - English\n",
    "#     text = re.sub(r'(;)( +)', ';#*^S^*#', text) # Tag sentence - English\n",
    "\n",
    "    text = re.sub(r'\\.\\n', '.#*^S^*#', text) # Tag sentence - English\n",
    "    text = re.sub(r'!\\n', '!#*^S^*#', text) # Tag sentence - English\n",
    "    text = re.sub(r'\\?\\n', '?#*^S^*#', text) # Tag sentence - English\n",
    "#     text = re.sub(r';\\n', ';#*^S^*#', text) # Tag sentence - English\n",
    "    \n",
    "    # merge two or more sentence-Tags -> 1 Sentence-Tag\n",
    "#     text = re.sub(r'(\\W?(\\#\\*\\^S\\^\\*\\#))+', '。#*^S^*#', text) # temp remove on 2017 Mar 24\n",
    "\n",
    "    # remove a Sentence-Tag immediately before an ending ”\n",
    "    text = re.sub(r'\\#\\*\\^S\\^\\*\\#”', '”', text) # Chinese \"\n",
    "    text = re.sub(r'\\#\\*\\^S\\^\\*\\#\\'', '\\'', text) # English '\n",
    "    text = re.sub(r'\\#\\*\\^S\\^\\*\\#\"', '\"', text) # English \"\n",
    "    \n",
    "    # remove a Sentence-Tag immediately before a Paragraph-Tag\n",
    "    text = re.sub(r'(\\#\\*\\^S\\^\\*\\#)( *)(\\#\\*\\^P\\^\\*\\#)', '#*^P^*#', text) \n",
    "    \n",
    "    text = re.sub(r'(\\#\\*\\^P\\^\\*\\#)+', '#*^P^*#', text) # merge two or more Paragraph-Tags -> 1 Paragraph-Tag\n",
    "\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content_format = linebreak_conversion_win_linux(content)\n",
    "content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content_format = tag_paragraph(content_format)\n",
    "content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_format = clean_some_whitespace_1(content_format)\n",
    "content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content_format = tag_sentence(content_format)\n",
    "content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_format = clean_some_whitespace_2(content_format)\n",
    "content_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Transfer tagged text to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split a text into paragraphs\n",
    "def split_article_to_paragraphs(text):\n",
    "#     text = text.replace(\"#*^P^*#\", \"#*^S^*#\") # convert Paragraph-Tag        \n",
    "    return text.split(\"#*^P^*#\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split a paragraph into sentences\n",
    "def split_paragraph_to_sentences(text):\n",
    "#     text = text.replace(\"#*^P^*#\", \"#*^S^*#\") # convert Paragraph-Tag        \n",
    "    return text.split(\"#*^S^*#\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1st loop Paragraphs list, 2nd loop Sentences list\n",
    "# create a few new columns, then write into dataframe, together with original Sentence string\n",
    "\n",
    "# define empty dataframe:\n",
    "df_article = pd.DataFrame(columns=('sentence', \n",
    "                                   'word_count', # sentence word count, including punctuations \n",
    "                                   'sentence_id', # unique sentence s/n within an article\n",
    "                                   'sentence_id_paragraph',  # sentence s/n within a paragraph \n",
    "                                   'paragraph_id', \n",
    "                                   'class_rank', \n",
    "                                   'score_word', # score based on word tf-idf\n",
    "                                   'score_sentence', # score based on intersection of sentence pairs\n",
    "                                   'score_word_norm', # Normalized score\n",
    "                                   'score_sentence_norm', # Normalized score\n",
    "                                   'score',\n",
    "                                  ))\n",
    "df_sentence_id = 0\n",
    "\n",
    "# split_article_to_paragraphs:\n",
    "article_paragraphs = split_article_to_paragraphs(content_format)\n",
    "\n",
    "for i in range(0, len(article_paragraphs)):\n",
    "    # split_paragraph_to_sentences:\n",
    "    article_paragraphs_sentences = split_paragraph_to_sentences(article_paragraphs[i].strip())\n",
    "\n",
    "    for j in range(0, len(article_paragraphs_sentences)):\n",
    "        if article_paragraphs_sentences[j].strip() != '':\n",
    "            df_sentence_id = df_sentence_id + 1\n",
    "            # write to dataframe:\n",
    "            df_article.loc[len(df_article)] = [article_paragraphs_sentences[j].strip(), \n",
    "                                               len(article_paragraphs_sentences[j].strip()), \n",
    "                                               df_sentence_id, \n",
    "                                               j+1, \n",
    "                                               i+1, \n",
    "                                               '', \n",
    "                                               '', \n",
    "                                               '', \n",
    "                                               '', \n",
    "                                               '', \n",
    "                                               '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure no empty sentences:\n",
    "print('Number of empty sentences in dataframe: %d ' % len(df_article[df_article['sentence'] == '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_article = df_article.sort_values(by=['sentence'], \n",
    "#                                     ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assume the 1st sentence as Title of Article\n",
    "\n",
    "title = df_article['sentence'][0]\n",
    "print('Title of Article : ', title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate importance score for each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional Reference] word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KudosData_word_tokenizer\n",
    "df_article['sentence_tokenized'] = df_article['sentence'].apply(lambda x: KudosData_word_tokenizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional Reference] Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KudosData_word_count\n",
    "df_article['sentence_tf'] = df_article['sentence'].apply(lambda x: KudosData_word_count(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional Reference] TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = df_article['sentence_tokenized']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# my_stopword_list = ['and','to','the','of', 'in']\n",
    "#vectorizer = TfidfVectorizer(stop_words=my_stopword_list)\n",
    "\n",
    "# choice of no nomalization of tfidf output (not recommended)\n",
    "#vectorizer = TfidfVectorizer(norm=None)\n",
    "\n",
    "# TF-IDF score\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# IDF score\n",
    "idf_dict = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "# TF is in df_article[['sentence_tf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 把TF-iDF数值赋予相对应的词组\n",
    "tfidf = tfidf.tocsr()\n",
    "\n",
    "n_docs = tfidf.shape[0]\n",
    "tfidftables = [{} for _ in range(n_docs)]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, j in zip(*tfidf.nonzero()):\n",
    "    tfidftables[i][terms[j]] = tfidf[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Document-Term-Matrix's TF-IDF matrix size:\n",
    "print ('This tfidf matrix is a very large table: [ %d rows/docs X %d columns/words ]' \n",
    "       % (tfidf.shape[0], tfidf.shape[1]))\n",
    "print ('It contains %d eliments: one score per word per document !'\n",
    "       % (tfidf.shape[0] * tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add tfidf score into dataframe \n",
    "df_article['tfidf'] = tfidftables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_article[['sentence', 'sentence_tokenized', 'tfidf']].head()\n",
    "df_article[['sentence', 'sentence_tokenized', 'sentence_tf', 'tfidf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring (1)\n",
    "### Calculate score_word for each sentence, based on sentence word_count tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiment: use tf-idf and len(sentence_tokenized) to calculate score\n",
    "# tmp_mean = tmp_sum / len(df_article['sentence_tokenized'][i])\n",
    "\n",
    "for i in range(0,len(df_article)):\n",
    "    if len(df_article['tfidf'][i]) == 0:\n",
    "        df_article['score_word'][i] = 0\n",
    "    else:\n",
    "        tmp_sum = 0\n",
    "        for key, values in df_article['tfidf'][i].items():\n",
    "            tmp_sum += values\n",
    "        \n",
    "        tmp_mean = tmp_sum / len(df_article['sentence_tokenized'][i])\n",
    "        df_article['score_word'][i] = tmp_mean \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring (2)\n",
    "### Calculate score_sentence for each sentence, based on pair-wise sentence comparison/intersection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Caculate raw intersection score between pair of two sentences, from df_article['sentence_tokenized']\n",
    "def sentences_intersection(sent1tokenized, sent2tokenized):\n",
    "    # www.KudosData.com - Chinese\n",
    "    # split the sentence into words/tokens\n",
    "    s1 = set(sent1tokenized.split(\" \"))\n",
    "    s2 = set(sent2tokenized.split(\" \"))\n",
    "\n",
    "    # If there is not intersection, just return 0\n",
    "    if (len(s1) + len(s2)) == 0:\n",
    "        print('# If there is not intersection, just return 0')\n",
    "        return 0\n",
    "\n",
    "    # Normalize the result by the average number of words\n",
    "    return len(s1.intersection(s2)) / ((len(s1) + len(s2)) / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below step runs long time... Tuning needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate important score of every pair of sentences\n",
    "\n",
    "n = len(df_article['sentence_tokenized'])\n",
    "        \n",
    "# [Sam python 2.7 -> 3.4] values = [[0 for x in xrange(n)] for x in xrange(n)]\n",
    "df_score_raw_values = [[0 for x in range(n)] for x in range(n)]\n",
    "for i in range(0, n):\n",
    "    for j in range(0, n):\n",
    "        df_score_raw_values[i][j] = sentences_intersection(df_article['sentence_tokenized'][i], \n",
    "                                                           df_article['sentence_tokenized'][j])\n",
    "\n",
    "# The score of a sentence is the sum of all its intersection\n",
    "sentences_dic = {}\n",
    "\n",
    "for i in range(0, n):\n",
    "    df_score = 0\n",
    "    for j in range(0, n):\n",
    "        if i == j:\n",
    "            continue\n",
    "        df_score += df_score_raw_values[i][j]\n",
    "    df_article['score_sentence'][i] = df_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data (Internal use,  not for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is to check if there is sentence with zero valid/real word\n",
    "\n",
    "# df_article[df_article['word_count'] == None]\n",
    "df_article[df_article['word_count'] <= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 图表显示：\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'图')  \n",
    "plt.xlabel(u'X坐标：Sentence word_count')  \n",
    "plt.ylabel(u'Y坐标：Sentence frequency')  \n",
    "# df_article['word_count'].value_counts().sort_values(ascending=False).plot(kind='bar', color='green')\n",
    "df_article['word_count'].hist(bins=100)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 图表显示：\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'图')  \n",
    "plt.xlabel(u'X坐标：Paragraph_id')  \n",
    "plt.ylabel(u'Y坐标：Sentence frequency')  \n",
    "df_article['paragraph_id'].hist(bins=100)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 图表显示：\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'图')  \n",
    "plt.xlabel(u'X坐标：sentence_id_paragraph')  \n",
    "plt.ylabel(u'Y坐标：Sentence frequency')  \n",
    "df_article['sentence_id_paragraph'].hist(bins=100)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 图表显示：\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'图')  \n",
    "plt.xlabel(u'X坐标：score_word')  \n",
    "plt.ylabel(u'Y坐标：frequency')  \n",
    "df_article['score_word'].hist(bins = 100)\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "#plt.xlim(0,0.5)\n",
    "#plt.ylim(0,0.5)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 图表显示：\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'图')  \n",
    "plt.xlabel(u'X坐标：score_sentence')  \n",
    "plt.ylabel(u'Y坐标：frequency')  \n",
    "df_article['score_sentence'].hist(bins = 100)\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "#plt.xlim(0,0.5)\n",
    "#plt.ylim(0,0.5)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_article[(df_article['score_word'] > 0.15) & (df_article['score_word'] < 0.25)]\n",
    "# df_article[(df_article['score_word'] > 0.2)].sort_values(by=['score_sentence', 'score_word'], ascending=[False, False,])\n",
    "# df_article[(df_article['score_sentence'] > 250)].sort_values(by=['score_word', 'score_sentence'], ascending=[False, False,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# log(score_word)\n",
    "df_article['score_word_log'] = np.log(df_article['score_word'].astype('float64') + \n",
    "                                      df_article[df_article['score_word'] >0 ]['score_word'].min()/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize score_word_log - Zero mean, unit variance\n",
    "\n",
    "# df_article['score_word_norm'] = (df_article['score_word'] - df_article['score_word'].mean()) / df_article['score_word'].std()\n",
    "df_article['score_word_norm'] = (df_article['score_word_log'] - df_article['score_word_log'].mean()) / df_article['score_word_log'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article['score_word_norm'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize score_sentence - Zero mean, unit variance\n",
    "\n",
    "df_article['score_sentence_norm'] = (df_article['score_sentence'] - df_article['score_sentence'].mean()) / df_article['score_sentence'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article['score_sentence_norm'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate class_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score integration\n",
    "# df_article['score'] = (df_article['score_sentence_norm'] + df_article['score_word_norm']) / 2\n",
    "\n",
    "# Sam Gu: 23 Mar 2017 - Experiment found that the score_word, which is based on tf-idf, doesn't seem to work well.\n",
    "#                       score_word     tends to favor short sentences\n",
    "#                       score_sentence tends to favor long  sentences\n",
    "#                       Hence, here we use score_sentence only for final scoring.\n",
    "\n",
    "# df_article['score'] = (df_article['score_word'] + df_article['score_sentence'] ) / 2\n",
    "df_article['score'] = df_article['score_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Min-Max normalization:\n",
    "df_article['score'] = (df_article['score'] - df_article['score'].min()) / (df_article['score'].max() -df_article['score'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article['score'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sort firstly\n",
    "df_article = df_article.sort_values(by=['paragraph_id', 'score'], ascending=[True, False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below step runs long time... Tuning needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Class_Rank\n",
    "\n",
    "current_class_rank = 0\n",
    "current_paragraph_id = 0\n",
    "\n",
    "for i in range(0, len(df_article)):\n",
    "    if df_article['paragraph_id'][i] != current_paragraph_id: # change of Paragraph, thus reset class_rank\n",
    "        current_class_rank = 1\n",
    "        current_paragraph_id = df_article['paragraph_id'][i]\n",
    "    else:\n",
    "        current_class_rank = current_class_rank + 1\n",
    "        \n",
    "    df_article['class_rank'][i] = current_class_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sort Dataframe to 'result lookup mode'\n",
    "df_article = df_article.sort_values(by=['class_rank', 'score', 'paragraph_id', 'sentence_id'], \n",
    "                                    ascending=[True, False, True, True]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article[['sentence',\n",
    "           'paragraph_id',\n",
    "           'sentence_id_paragraph',\n",
    "           'class_rank',\n",
    "           'score',\n",
    "           'sentence_tokenized'\n",
    "          ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_article[(df_article['score'] == 0) | (df_article['score'] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract results based on user parameters:\n",
    "* Max number of words\n",
    "* % of original number of words\n",
    "* Max lines of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a dataframe copy\n",
    "# Currently, the two dataframes are exactly the same.\n",
    "df_article_internal = pd.DataFrame.copy(df_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words_original_article = df_article['sentence'].map(len).sum()\n",
    "total_words_internal_article = df_article_internal['sentence'].map(len).sum()\n",
    "# total_words_article_summary  = df_article_final['sentence'].map(len).sum()\n",
    "\n",
    "# print('total_words_original_article : ', total_words_original_article)\n",
    "# print('total_words_internal_article : ', total_words_internal_article)\n",
    "# print('total_words_article_summary  : ', total_words_article_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sam Gu: experiment shows no major improvement to use code in this block:\n",
    "\n",
    "'''\n",
    "\n",
    "# Heuristic cleaning:\n",
    "# 1.Remove sentences, which has only one valid word. \n",
    "# 2.Remove paragraph, which has only single sentence.\n",
    "\n",
    "# 1.\n",
    "df_article_internal = df_article_internal[df_article_internal['sentence_tokenized'].map(len) > 1]\n",
    "print('*** www.KudosData.com *** Removed number of sentences, which has only one valid word : %d'\n",
    "      % (len(df_article) - len(df_article_internal)))\n",
    "\n",
    "# 2.\n",
    "df_article_internal_paragraph = df_article_internal['paragraph_id'].value_counts().to_frame(name = 'sentence_count')\n",
    "df_article_internal_paragraph = df_article_internal_paragraph[df_article_internal_paragraph['sentence_count'] > 1]\n",
    "valid_paragraph_id = df_article_internal_paragraph.index.tolist()\n",
    "df_article_internal = df_article_internal[df_article_internal['paragraph_id'].isin(valid_paragraph_id)] \n",
    "print('*** www.KudosData.com *** Removed number of sentences in total : %d' % (len(df_article) - len(df_article_internal)))\n",
    "\n",
    "# sort Dataframe to 'result lookup mode'\n",
    "df_article_internal = df_article_internal.sort_values(by=['class_rank', 'score', 'paragraph_id', 'sentence_id'], \n",
    "                                    ascending=[True, False, True, True]).reset_index(drop=True)\n",
    "# Above sort a must sort !!! for below processing:\n",
    "# Loop Dataframe, accumulate length of sentences, stop when parm_max_word reached, return the index(), cut dataframe to display\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Accept user parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# valid range: >= 0\n",
    "parm_max_word = 200\n",
    "\n",
    "# valid range: >= 0\n",
    "parm_max_sentence = 10\n",
    "\n",
    "# valid range: [0, 100%]\n",
    "parm_max_percent = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Validation of user parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (isinstance(parm_max_word, int) | isinstance(parm_max_word, float)):\n",
    "    if parm_max_word >= 0:\n",
    "        print('!1! valid input parm_max_word : ', parm_max_word)\n",
    "    else:\n",
    "        print('!2! Invalid input parm_max_word : ', parm_max_word)    \n",
    "else:\n",
    "    print('!3! Invalid input parm_max_word : ', parm_max_word)\n",
    "\n",
    "if (isinstance(parm_max_sentence, int) | isinstance(parm_max_sentence, float)):\n",
    "    if parm_max_sentence >= 0:\n",
    "        print('!1! valid input parm_max_sentence : ', parm_max_sentence)\n",
    "    else:\n",
    "        print('!2! Invalid input parm_max_sentence : ', parm_max_sentence)    \n",
    "else:\n",
    "    print('!3! Invalid input parm_max_sentence : ', parm_max_sentence)\n",
    "\n",
    "if (isinstance(parm_max_percent, int) | isinstance(parm_max_percent, float)):\n",
    "    if parm_max_percent >= 0:\n",
    "        print('!1! valid input parm_max_percent : ', parm_max_percent)\n",
    "    else:\n",
    "        print('!2! Invalid input parm_max_percent : ', parm_max_percent)    \n",
    "else:\n",
    "    print('!3! Invalid input parm_max_percent : ', parm_max_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cut by parm_max_percent\n",
    "\n",
    "# Loop Dataframe, accumulate length of sentences, stop when parm_max_word reached, return the index(), cut dataframe to display\n",
    "\n",
    "sum_current_word = 0\n",
    "cut_index = len(df_article_internal['sentence'])\n",
    "\n",
    "# print('Start loop...')\n",
    "for s in range(0, len(df_article_internal['sentence'])):\n",
    "#     print('s : %d' % s)\n",
    "    if sum_current_word / total_words_original_article <= parm_max_percent:\n",
    "        sum_current_word += len(df_article_internal['sentence'][s])\n",
    "    else:\n",
    "#         stop, return index number\n",
    "        cut_index = s - 1\n",
    "        sum_current_word -= len(df_article_internal['sentence'][s-1])\n",
    "\n",
    "#         print('To break')\n",
    "        break\n",
    "\n",
    "# print('End loop')\n",
    "sum_current_percent = sum_current_word / total_words_original_article\n",
    "print('---------- cut by parm_max_percent :')\n",
    "print('sum_current_word  / total_words_original_article:', sum_current_percent)\n",
    "print('cut_index : ', cut_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cut by parm_max_word\n",
    "\n",
    "# Loop Dataframe, accumulate length of sentences, stop when parm_max_word reached, return the index(), cut dataframe to display\n",
    "\n",
    "sum_current_word = 0\n",
    "cut_index = len(df_article_internal['sentence'])\n",
    "\n",
    "# print('Start loop...')\n",
    "for s in range(0, len(df_article_internal['sentence'])):\n",
    "#     print('s : %d' % s)\n",
    "    if sum_current_word <= parm_max_word:\n",
    "        sum_current_word += len(df_article_internal['sentence'][s])\n",
    "    else:\n",
    "#         stop, return index number\n",
    "        cut_index = s - 1\n",
    "        sum_current_word -= len(df_article_internal['sentence'][s-1])\n",
    "#         print('To break')\n",
    "        break\n",
    "\n",
    "# print('End loop')\n",
    "print('---------- cut by parm_max_word :')\n",
    "print('sum_current_word :', sum_current_word)\n",
    "print('cut_index : ', cut_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cut by parm_max_sentence\n",
    "\n",
    "cut_index = parm_max_sentence\n",
    "\n",
    "print('---------- cut by parm_max_sentence :')\n",
    "print('cut_index : ', cut_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract top number of sentences as summary, based on: cut_index\n",
    "df_article_final = df_article_internal[0:cut_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sort by original sentence order \n",
    "df_article_final = df_article_final.sort_values(by=['sentence_id'], ascending=[True])\n",
    "df_article_final[['sentence', 'paragraph_id', 'sentence_id_paragraph', 'class_rank', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total_words_original_article = df_article['sentence'].map(len).sum()\n",
    "# total_words_internal_article = df_article_internal['sentence'].map(len).sum()\n",
    "total_words_article_summary  = df_article_final['sentence'].map(len).sum()\n",
    "\n",
    "print('total_words_original_article : ', total_words_original_article)\n",
    "print('total_words_internal_article : ', total_words_internal_article)\n",
    "print('total_words_article_summary  : ', total_words_article_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('\\n'.join(list(df_article_final['sentence'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with io.open('output_topic_summary.txt','w',encoding='utf8') as f:\n",
    "#     f.write(\"Original Length : %s\" % total_words_original_article)\n",
    "    f.write(\"No. Paragraphs  : %d\" % df_article_internal['paragraph_id'].max())\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Original Length : %s\" % total_words_internal_article)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Summary  Length : %s\" % total_words_article_summary)\n",
    "    f.write(\"\\n\")\n",
    "#     f.write(\"Summary  Ratio  : %s %%\" % (100 * (sum_current_word / total_words_original_article)))\n",
    "    f.write(\"Summary  Ratio  : %.2f %%\" % (100 * (total_words_article_summary / total_words_internal_article)))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Title of Article: %s\" % title)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write('\\n'.join(list(df_article_final['sentence'])))\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
