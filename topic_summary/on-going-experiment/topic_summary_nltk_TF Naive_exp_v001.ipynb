{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization - Support Both English & Chinese Inputs\n",
    "### www.KudosData.com\n",
    "#### By: Sam GU Zhan\n",
    "#### March, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A possible error: Failed loading english.pickle with nltk.data.load\n",
    "\n",
    "Resource 'tokenizers/punkt/english.pickle' not found.  Please use the NLTK Downloader to obtain the resource:  >>>\n",
    "\n",
    "http://stackoverflow.com/questions/4867197/failed-loading-english-pickle-with-nltk-data-load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     G:\\Tool_PGM\\Anaconda3\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_cut = 0.01\n",
    "min_cut = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FrequencySummarizer:\n",
    "    def __init__(self, min_cut=0.1, max_cut=0.9):\n",
    "        \"\"\"\n",
    "        Initilize the text summarizer.\n",
    "        Words that have a frequency term lower than min_cut \n",
    "        or higer than max_cut will be ignored.\n",
    "        \"\"\"\n",
    "        self._min_cut = min_cut\n",
    "        self._max_cut = max_cut \n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "    def _compute_frequencies(self, word_sent):\n",
    "            \"\"\" \n",
    "               Compute the frequency of each of word.\n",
    "               Input: \n",
    "                word_sent, a list of sentences already tokenized.\n",
    "               Output: \n",
    "                freq, a dictionary where freq[w] is the frequency of w.\n",
    "             \"\"\"\n",
    "            freq = defaultdict(int)\n",
    "            for s in word_sent:\n",
    "                 for word in s:\n",
    "                    if word not in self._stopwords:\n",
    "                         freq[word] += 1\n",
    "        # frequencies normalization and fitering\n",
    "            m = float(max(freq.values()))\n",
    "# Python 2->3 conversion            \n",
    "#             for w in freq.keys():\n",
    "            for w in list(freq):\n",
    "                freq[w] = freq[w]/m\n",
    "                if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
    "                      del freq[w]\n",
    "            return freq\n",
    "    \n",
    "    def summarize(self, text, n):\n",
    "        \"\"\"\n",
    "        Return a list of n sentences \n",
    "        which represent the summary of text.\n",
    "        \"\"\"\n",
    "        sents = sent_tokenize(text)\n",
    "        assert n <= len(sents)\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "        self._freq = self._compute_frequencies(word_sent)\n",
    "        ranking = defaultdict(int)\n",
    "        for i,sent in enumerate(word_sent):\n",
    "            for w in sent:\n",
    "                if w in self._freq:\n",
    "                    ranking[i] += self._freq[w]\n",
    "        sents_idx = self._rank(ranking, n)    \n",
    "        return [sents[j] for j in sents_idx]\n",
    "\n",
    "    def _rank(self, ranking, n):\n",
    "        \"\"\" return the first n sentences with highest ranking \"\"\"\n",
    "        return nlargest(n, ranking, key=ranking.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import urllib2\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_only_text(url):\n",
    "    \"\"\" \n",
    "    return the title and the text of the article\n",
    "    at the specified url\n",
    "    \"\"\"\n",
    "#     page = urllib2.urlopen(url).read().decode('utf8')\n",
    "    page = urlopen(url).read().decode('utf8')\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
    "    return soup.title.text, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_xml = urlopen('http://feeds.bbci.co.uk/news/rss.xml').read()\n",
    "feed = BeautifulSoup(feed_xml.decode('utf8'), \"lxml\")\n",
    "# to_summarize = map(lambda p: p.text, feed.find_all('guid'))\n",
    "to_summarize = list(map(lambda p: p.text, feed.find_all('guid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.bbc.co.uk/news/world-us-canada-39388815',\n",
       " 'http://www.bbc.co.uk/news/uk-39377883',\n",
       " 'http://www.bbc.co.uk/news/world-middle-east-39383989',\n",
       " 'http://www.bbc.co.uk/news/uk-politics-39380606',\n",
       " 'http://www.bbc.co.uk/news/entertainment-arts-39355410',\n",
       " 'http://www.bbc.co.uk/news/world-europe-39383988',\n",
       " 'http://www.bbc.co.uk/news/business-39383535',\n",
       " 'http://www.bbc.co.uk/news/education-39381899',\n",
       " 'http://www.bbc.co.uk/news/uk-england-hereford-worcester-39380779',\n",
       " 'http://www.bbc.co.uk/news/uk-england-stoke-staffordshire-39379053',\n",
       " 'http://www.bbc.co.uk/news/world-middle-east-39382250',\n",
       " 'http://www.bbc.co.uk/news/uk-england-kent-39377133',\n",
       " 'http://www.bbc.co.uk/news/uk-39385125',\n",
       " 'http://www.bbc.co.uk/news/blogs-the-papers-39388352',\n",
       " 'http://www.bbc.co.uk/news/uk-39355108',\n",
       " 'http://www.bbc.co.uk/news/magazine-39346157',\n",
       " 'http://www.bbc.co.uk/sport/39360295',\n",
       " 'http://www.bbc.co.uk/news/uk-england-lancashire-39386044',\n",
       " 'http://www.bbc.co.uk/news/world-us-canada-39375228',\n",
       " 'http://www.bbc.co.uk/news/uk-39377966',\n",
       " 'http://www.bbc.co.uk/news/entertainment-arts-39381159',\n",
       " 'http://www.bbc.co.uk/news/blogs-trending-39379149',\n",
       " 'http://www.bbc.co.uk/news/technology-39380884',\n",
       " 'http://www.bbc.co.uk/news/10318089',\n",
       " 'http://www.bbc.co.uk/news/uk-39380049',\n",
       " 'http://www.bbc.co.uk/news/uk-39363933',\n",
       " 'http://www.bbc.co.uk/news/magazine-39366596',\n",
       " 'http://www.bbc.co.uk/news/magazine-37024334',\n",
       " 'http://www.bbc.co.uk/news/world-asia-39361984',\n",
       " 'http://www.bbc.co.uk/news/world-us-canada-39371204',\n",
       " 'http://www.bbc.co.uk/news/business-39371460',\n",
       " 'http://www.bbc.co.uk/news/health-39353379',\n",
       " 'http://www.bbc.co.uk/sport/football/39388290',\n",
       " 'http://www.bbc.co.uk/sport/football/39300143',\n",
       " 'http://www.bbc.co.uk/sport/39387363',\n",
       " 'http://www.bbc.co.uk/news/uk-39385005',\n",
       " 'http://www.bbc.co.uk/news/uk-39382091',\n",
       " 'http://www.bbc.co.uk/news/uk-39379808',\n",
       " 'http://www.bbc.co.uk/news/uk-39376643',\n",
       " 'http://www.bbc.co.uk/news/uk-39377842']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Below step seems require internet access !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Trump blames Democrats for failed healthcare bill - BBC News\n",
      "* Speaking to the Washington Post, Mr Trump said \"We couldn't get one Democratic vote, and we were a little bit shy, very little, but it was still a little bit shy, so we pulled it.\"\n",
      "* \"We have to let Obamacare go its own way for a little while,\" he told reporters at the Oval Office, adding that if the Democrats were \"civilised and came together\" the two parties could work out a \"great healthcare bill\".\n",
      "----------------------------------\n",
      "London attack: Police appeal for information on Khalid Masood - BBC News\n",
      "* Those still in custody are: Earlier, in appealing for information from the public, Mark Rowley, of the Metropolitan Police, said they would investigate whether Masood \"acted totally alone inspired by terrorist propaganda, or if others have encouraged, supported or directed him\".\n",
      "* Mr Toumi said Masood had been \"friendly and smiley\", while the hotel receptionist noted on their system he was a \"nice guest\".\n"
     ]
    }
   ],
   "source": [
    "fs = FrequencySummarizer()\n",
    "for article_url in to_summarize[:2]:\n",
    "    title, text = get_only_text(article_url)\n",
    "    print ('----------------------------------')\n",
    "    print (title)\n",
    "    for s in fs.summarize(text, 2):\n",
    "        print ('*',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
