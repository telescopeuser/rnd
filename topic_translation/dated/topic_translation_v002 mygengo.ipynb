{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Translation - Support Both English & Chinese Inputs\n",
    "### www.KudosData.com\n",
    "#### By: Sam GU Zhan\n",
    "#### March, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=UTF-8\n",
    "from __future__ import division\n",
    "import re\n",
    "\n",
    "# Python2 unicode & float-division support:\n",
    "# from __future__ import unicode_literals, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "\n",
    "# 中文字符和语言处理库\n",
    "import jieba\n",
    "\n",
    "# 机器学习库 sklearn 分类学习模型库\n",
    "#from sklearn import linear_model\n",
    "from sklearn.feature_extraction import DictVectorizer # 数据结构变换：把 Dict 转换为 稀疏矩阵\n",
    "# from sklearn.linear_model import LogisticRegression  # 逻辑回归分类模型\n",
    "# from sklearn.pipeline import make_pipeline # 封装机器学习模型流程\n",
    "# from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# 中文显示设置\n",
    "from pylab import *  \n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei'] # 指定默认字体  \n",
    "mpl.rcParams['axes.unicode_minus'] = False # 解决保存图像是负号'-'显示为方块的问题 \n",
    "mpl.rcParams['font.size'] = 14 # 设置字体大小\n",
    "\n",
    "np.random.seed(88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python3\n",
    "# 中文分词功能小函数， 输出 字符串， 各词组由空格分隔\n",
    "def KudosData_word_tokenizer(foo):\n",
    "    # remove lead & tail spaces firstly:\n",
    "    foo = foo.strip()\n",
    "    seg_token = jieba.cut(str(foo), cut_all=True)\n",
    "    seg_str = str(' '.join(seg_token)).strip()\n",
    "\n",
    "    return seg_str\n",
    "# Python2\n",
    "# 中文分词功能小函数， 输出 字符串， 各词组由空格分隔\n",
    "# def KudosData_word_tokenizer(foo):\n",
    "#     seg_token = jieba.cut(foo, cut_all=True)\n",
    "#     seg_str = ' '.join(seg_token)\n",
    "#     return seg_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python3\n",
    "# 中文分词功能小函数， 输出 字符串， 各词组由空格分隔\n",
    "def KudosData_word_count(foo):\n",
    "    # remove lead & tail spaces firstly:\n",
    "    foo = foo.strip()\n",
    "    seg_token = jieba.cut(str(foo), cut_all=True)\n",
    "    seg_str = str(' '.join(seg_token)).strip()\n",
    "    seg_count = pd.value_counts(str(seg_str).lower().split(' '))\n",
    "    seg_count = seg_count.to_dict() \n",
    "    seg_count.pop('', None) # remove EMPTY dict key: ''\n",
    "#     输出 dictionary： { key 词组， value 计数 }\n",
    "    #     return seg_count.to_dict()\n",
    "    return seg_count\n",
    "\n",
    "# Python2\n",
    "# 中文分词功能小函数， 输出 dictionary： { key 词组， value 计数 }\n",
    "# def KudosData_word_count(foo):\n",
    "#     seg_token = jieba.cut(foo, cut_all=True)\n",
    "#     seg_str = '^'.join(seg_token)\n",
    "#     seg_count = pd.value_counts(seg_str.lower().split('^'))\n",
    "#     return seg_count.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# process Unicode text input\n",
    "with io.open('output_topic_summary.txt','r',encoding='utf8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "title = '''\n",
    "<Dummy Title>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No. Paragraphs  : 213\\nOriginal Length : 32378\\nSummary  Length : 143\\nSummary  Ratio  : 0.44 %\\n\\nTitle of Article: 《黄金时代》 王小波\\n\\n可是她说，快，混蛋，还拧我的腿。\\n\\nGolden Age 《黄金时代》\\nWang xiaobo 王小波\\n \\n \\none  一  \\n \\nWhen I was twenty years old, I was \"jumping\" in Yunnan. \\u3000\\u3000我二十一岁时，正在云南“插队”。\\n Chen Ching-yang was twenty-six years old, just when I jumped into the team as a doctor. 陈清扬当时二十六岁，就在我插队的地方当医生。\\n I am under the foot of the fourteen teams in the mountains, she fifteen teams in the mountains. 我在山下十四队，她在山上十五队。\\n One day she came down from the mountain, and I discussed her not the \\'broken shoes\\'. 有一天她从山上下来，和我讨论她不是‘破鞋’的问题。\\n At that time I did not know her, can only say that little know. 那时我还不大认识她，只能说有一点知道。\\n The thing she wants to discuss is this: though all the people say she is a broken shoe, but she thinks she is not. 她要讨论的事是这祥的：虽然所有的人都说她是一个破鞋，但她以为自己不是的。\\n Because the broken shoes to steal Han, and she did not laugh Han. 因为破鞋偷汉，而她没有愉过汉。\\n Although her husband had lived for one year, she had not stolen. 虽然她丈夫已经住了一年监狱，但她没有偷过汉。\\n Before this did not steal Han. 在此之前也未偷过汉。\\n So she simply do not understand why people say she is broken shoes. 所以她简直不明白，人们为什么要说她是破鞋。\\n If I want to comfort her, it is not difficult. 如果我要安慰她，并不困难。\\n I can logically prove that she is not breaking shoes. 我可以从逻辑上证明她不是破鞋。\\n If Chen Qingyang is broken shoes, that Chen Qingyang steal Han, then at least one of them stolen for it. 如果陈清扬是破鞋，即陈清扬偷汉，则起码有一个某人为其所偷。\\n Now can not point out someone, so Chen Qingyang can not be established. 如今不能指出某人，所以陈清扬偷汉不能成立。\\n But I say, Chen Qiangyang is broken shoes, and this is no doubt. 但是我偏说，陈清扬就是破鞋，而且这一点毋庸置疑。\\n '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MyGengo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-095d083c8ce5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# this package is paid API. And it does not support python3+ yet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmygengo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMyGengo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mG:\\Tool_PGM\\Anaconda3\\lib\\site-packages\\mygengo\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmygengo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMyGengo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMyGengoError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMyGengoAuthError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MyGengo'"
     ]
    }
   ],
   "source": [
    "# this package is paid API. And it does not support python3+ yet\n",
    "from mygengo import MyGengo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gengo = MyGengo(\n",
    "    public_key = 'your_mygengo_api_key',\n",
    "    private_key = 'your_mygengo_private_key',\n",
    "    sandbox = False, # possibly False, depending on your dev needs\n",
    ")\n",
    "\n",
    "translation = gengo.postTranslationJob(job = {\n",
    "    'type': 'text', # REQUIRED. Type to translate, you'll probably always put 'text' here (for now ;)\n",
    "    'slug': 'Translating Chinese to English with the myGengo API', # REQUIRED. For storing on the myGengo side\n",
    "    'body_src': '我們今天要去那裏嗎', # REQUIRED. The text you're translating. ;P\n",
    "    'lc_src': 'zh', # REQUIRED. source_language_code (see getServiceLanguages() for a list of codes)  \n",
    "    'lc_tgt': 'en', # REQUIRED. target_language_code (see getServiceLanguages() for a list of codes)\n",
    "    'tier': 'standard', # REQUIRED. tier type (\"machine\", \"standard\", \"pro\", or \"ultra\")\n",
    "})\n",
    "\n",
    "# This will print out a machine translation; for your human translation, you can\n",
    "# poll and check often, or set up a URL for it to post the results to.\n",
    "print (translation['response']['job']['body_tgt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
